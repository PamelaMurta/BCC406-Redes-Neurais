{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["# Notebook 05: Comparative Analysis\n\n## Objectives\n1. Load results from both models\n2. Compare performance metrics\n3. Statistical significance testing\n4. Visualize comparisons\n5. Generate final report"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["import sys\nsys.path.append('..')\n\nfrom src.utils.helpers import load_config, load_results\nfrom src.models.random_forest import RandomForestSpeakerClassifier\nfrom src.models.cnn_1d import CNN1DSpeakerClassifier\nfrom src.evaluation.metrics import evaluate_model, compare_models, statistical_significance_test\nfrom src.evaluation.visualization import plot_model_comparison, plot_confusion_matrix\nfrom src.data.dataset import FeatureDataset\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nconfig = load_config('../config/config.yaml')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Load test features\nrf_test = FeatureDataset.load_pickle('../data/processed/test_aggregated.pkl')\nX_test_rf, y_test = rf_test.get_data()\n\ncnn_test = FeatureDataset.load_hdf5('../data/processed/test_sequential.h5')\nX_test_cnn, y_test_cnn = cnn_test.get_data()\n\nprint(f'RF test features: {X_test_rf.shape}')\nprint(f'CNN test features: {X_test_cnn.shape}')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Load models\nnum_speakers = len(set(y_test))\n\nrf_model = RandomForestSpeakerClassifier(num_speakers, config)\nrf_model.load('../models/random_forest_best.pkl')\n\ncnn_model = CNN1DSpeakerClassifier(num_speakers, X_test_cnn.shape[1:], config)\ncnn_model.load('../models/cnn_best.h5')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Evaluate both models\nrf_results = evaluate_model(rf_model, X_test_rf, y_test)\ncnn_results = evaluate_model(cnn_model, X_test_cnn, y_test_cnn)\n\nprint('Random Forest Results:')\nfor metric, value in rf_results['metrics'].items():\n    print(f'  {metric}: {value:.4f}')\n\nprint('\\nCNN 1D Results:')\nfor metric, value in cnn_results['metrics'].items():\n    print(f'  {metric}: {value:.4f}')"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Compare models\ncomparison_df = compare_models(\n    rf_results['predictions'],\n    cnn_results['predictions'],\n    y_test,\n    model1_name='Random Forest',\n    model2_name='CNN 1D'\n)\n\nprint('\\nModel Comparison:')\nprint(comparison_df.to_string(index=False))"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Plot comparison\nfig = plot_model_comparison(\n    comparison_df,\n    title='Random Forest vs CNN 1D - Performance Comparison',\n    save_path='../results/comparison/model_comparison.png'\n)\nplt.show()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Plot confusion matrices side by side\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\n\nimport seaborn as sns\nsns.heatmap(rf_results['confusion_matrix_normalized'], annot=True, fmt='.2f', \n            cmap='Blues', ax=axes[0])\naxes[0].set_title('Random Forest')\naxes[0].set_xlabel('Predicted')\naxes[0].set_ylabel('True')\n\nsns.heatmap(cnn_results['confusion_matrix_normalized'], annot=True, fmt='.2f', \n            cmap='Blues', ax=axes[1])\naxes[1].set_title('CNN 1D')\naxes[1].set_xlabel('Predicted')\naxes[1].set_ylabel('True')\n\nplt.tight_layout()\nplt.savefig('../results/comparison/confusion_matrices.png', dpi=300, bbox_inches='tight')\nplt.show()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Statistical significance test\nrf_correct = (rf_results['predictions'] == y_test).astype(float)\ncnn_correct = (cnn_results['predictions'] == y_test_cnn).astype(float)\n\nstat_test = statistical_significance_test(\n    rf_correct,\n    cnn_correct,\n    test='wilcoxon',\n    alpha=0.05\n)\n\nprint('\\nStatistical Significance Test:')\nprint(f\"Test: {stat_test['test']}\")\nprint(f\"P-value: {stat_test['p_value']:.4f}\")\nprint(f\"Significant: {stat_test['is_significant']}\")\nprint(f\"\\n{stat_test['interpretation']}\")"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Per-speaker comparison\nfig, ax = plt.subplots(figsize=(12, 6))\nx = np.arange(num_speakers)\nwidth = 0.35\n\nrf_per_speaker = [rf_results['per_speaker_accuracy'][i] for i in range(num_speakers)]\ncnn_per_speaker = [cnn_results['per_speaker_accuracy'][i] for i in range(num_speakers)]\n\nax.bar(x - width/2, rf_per_speaker, width, label='Random Forest', alpha=0.7)\nax.bar(x + width/2, cnn_per_speaker, width, label='CNN 1D', alpha=0.7)\n\nax.set_xlabel('Speaker')\nax.set_ylabel('Accuracy')\nax.set_title('Per-Speaker Accuracy Comparison')\nax.set_xticks(x)\nax.set_xticklabels([f'S{i}' for i in range(num_speakers)])\nax.legend()\nax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig('../results/comparison/per_speaker_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()"]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": ["# Final summary\nprint('=' * 60)\nprint('FINAL SUMMARY')\nprint('=' * 60)\nprint(f'\\nRandom Forest Accuracy: {rf_results[\"metrics\"][\"accuracy\"]:.4f}')\nprint(f'CNN 1D Accuracy: {cnn_results[\"metrics\"][\"accuracy\"]:.4f}')\nprint(f'\\nImprovement: {(cnn_results[\"metrics\"][\"accuracy\"] - rf_results[\"metrics\"][\"accuracy\"]) * 100:.2f}%')\nprint(f'\\nStatistical Significance: {\"Yes\" if stat_test[\"is_significant\"] else \"No\"}')\nprint(f'P-value: {stat_test[\"p_value\"]:.4f}')\nprint('\\n' + '=' * 60)"]
    }
  ],
  "metadata": {
    "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
