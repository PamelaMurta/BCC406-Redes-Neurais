# Notebook Completo para Google Colab - RF vs CNN para Identifica√ß√£o de Falantes

Este notebook implementa o pipeline completo de treinamento e compara√ß√£o entre Random Forest e CNN 1D para identifica√ß√£o de falantes usando o dataset VoxCeleb1.

## üìã √çndice
1. Configura√ß√£o do Ambiente
2. Download do Dataset VoxCeleb1
3. An√°lise Explorat√≥ria
4. Extra√ß√£o de Features
5. Treinamento Random Forest
6. Treinamento CNN 1D
7. An√°lise Comparativa
8. Resultados e Conclus√µes

---

## 1Ô∏è‚É£ Configura√ß√£o do Ambiente

```python
# Verificar GPU
!nvidia-smi

# Instalar depend√™ncias
!pip install librosa soundfile pydub scikit-learn tensorflow matplotlib seaborn tqdm pyyaml h5py joblib wget

# Clonar reposit√≥rio
!git clone https://github.com/seu-usuario/BCC406-Redes-Neurais.git
%cd BCC406-Redes-Neurais

# Importar bibliotecas
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import librosa
import tensorflow as tf
from pathlib import Path
from tqdm import tqdm
import pickle

print("‚úì Ambiente configurado")
print(f"TensorFlow: {tf.__version__}")
print(f"GPU dispon√≠vel: {tf.config.list_physical_devices('GPU')}")
```

---

## 2Ô∏è‚É£ Download do Dataset VoxCeleb1

```python
# Op√ß√£o 1: Download do conjunto de teste (menor, ~5GB)
!python scripts/baixar_voxceleb1.py

# OU

# Op√ß√£o 2: Upload manual do Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Copiar do Drive
!cp -r /content/drive/MyDrive/VoxCeleb1 data/raw/
```

```python
# Verificar estrutura do dataset
!ls -lh data/raw/
!find data/raw/ -name "*.wav" | head -20
```

---

## 3Ô∏è‚É£ An√°lise Explorat√≥ria

```python
# Carregar m√≥dulos do projeto
sys.path.append('.')
from src.data.dataset import SpeakerDataset
from src.utils.helpers import load_config, print_system_info

# Informa√ß√µes do sistema
print_system_info()

# Carregar configura√ß√£o
config = load_config('config/config.yaml')

# Carregar dataset
dataset = SpeakerDataset('data/raw/wav')
print(f"\n‚úì Dataset carregado:")
print(f"  Falantes: {dataset.get_num_speakers()}")
print(f"  Arquivos: {len(dataset.audio_files)}")
```

```python
# Visualizar distribui√ß√£o de falantes
import matplotlib.pyplot as plt
import seaborn as sns

# Criar DataFrame
data = []
for audio_file, label in zip(dataset.audio_files, dataset.labels):
    speaker_name = dataset.get_speaker_name(label)
    data.append({'speaker_id': speaker_name, 'label': label})

df = pd.DataFrame(data)
speaker_counts = df['speaker_id'].value_counts()

# Plotar distribui√ß√£o
plt.figure(figsize=(15, 5))
speaker_counts.head(20).plot(kind='bar', color='steelblue', alpha=0.7)
plt.xlabel('ID do Falante')
plt.ylabel('N√∫mero de Amostras')
plt.title('Distribui√ß√£o dos 20 Principais Falantes')
plt.grid(True, alpha=0.3, axis='y')
plt.tight_layout()
plt.show()

print(f"\nEstat√≠sticas:")
print(f"  M√©dia: {speaker_counts.mean():.1f}")
print(f"  Desvio: {speaker_counts.std():.1f}")
print(f"  M√≠n: {speaker_counts.min()}")
print(f"  M√°x: {speaker_counts.max()}")
```

```python
# Visualizar amostras de √°udio
import IPython.display as ipd

# Selecionar 3 falantes aleat√≥rios
falantes_exemplo = np.random.choice(dataset.get_num_speakers(), 3, replace=False)

for speaker_id in falantes_exemplo:
    indices = [i for i, l in enumerate(dataset.labels) if l == speaker_id]
    if indices:
        audio_file = dataset.audio_files[indices[0]]
        speaker_name = dataset.get_speaker_name(speaker_id)
        
        print(f"\n{'='*60}")
        print(f"Falante: {speaker_name}")
        print(f"Arquivo: {Path(audio_file).name}")
        print(f"{'='*60}")
        
        # Carregar e exibir √°udio
        y, sr = librosa.load(audio_file, sr=16000, duration=5.0)
        
        # Player de √°udio
        display(ipd.Audio(y, rate=sr))
        
        # Forma de onda
        plt.figure(figsize=(12, 3))
        librosa.display.waveshow(y, sr=sr)
        plt.title(f'Forma de Onda - {speaker_name}')
        plt.xlabel('Tempo (s)')
        plt.ylabel('Amplitude')
        plt.tight_layout()
        plt.show()
        
        # Espectrograma
        plt.figure(figsize=(12, 4))
        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)
        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz')
        plt.colorbar(format='%+2.0f dB')
        plt.title(f'Espectrograma - {speaker_name}')
        plt.tight_layout()
        plt.show()
```

---

## 4Ô∏è‚É£ Extra√ß√£o de Features

```python
# Dividir dataset
print("Dividindo dataset...")
train_idx, val_idx, test_idx = dataset.split_dataset(
    train_ratio=0.7,
    val_ratio=0.15,
    test_ratio=0.15,
    seed=42
)

print(f"‚úì Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}")
```

```python
# Fun√ß√£o de extra√ß√£o de features
def extrair_features(arquivo_audio, sr=16000, max_len=100):
    """Extrair MFCCs de um arquivo de √°udio"""
    try:
        # Carregar √°udio
        y, _ = librosa.load(arquivo_audio, sr=sr, duration=5.0)
        
        # Extrair 40 MFCCs
        mfccs = librosa.feature.mfcc(
            y=y, sr=sr, n_mfcc=40, 
            n_fft=2048, hop_length=512
        )
        
        # Ajustar para tamanho fixo
        if mfccs.shape[1] < max_len:
            mfccs = np.pad(mfccs, ((0, 0), (0, max_len - mfccs.shape[1])), 
                          mode='constant')
        else:
            mfccs = mfccs[:, :max_len]
        
        return mfccs.T  # (tempo, features)
    
    except Exception as e:
        print(f"Erro: {e}")
        return None

# Extrair features para cada conjunto
def carregar_features(indices, desc="Processando"):
    features = []
    labels = []
    
    for idx in tqdm(indices, desc=desc):
        audio_file, label = dataset[idx]
        feat = extrair_features(audio_file)
        
        if feat is not None:
            features.append(feat)
            labels.append(label)
    
    return np.array(features), np.array(labels)

print("\nExtraindo features...")
print("‚ö†Ô∏è  Isso pode levar alguns minutos dependendo do tamanho do dataset\n")

X_train, y_train = carregar_features(train_idx, "Train")
X_val, y_val = carregar_features(val_idx, "Val")
X_test, y_test = carregar_features(test_idx, "Test")

print(f"\n‚úì Features extra√≠das:")
print(f"  Train: {X_train.shape}")
print(f"  Val:   {X_val.shape}")
print(f"  Test:  {X_test.shape}")

# Salvar features (opcional)
!mkdir -p data/processed
np.save('data/processed/X_train.npy', X_train)
np.save('data/processed/y_train.npy', y_train)
np.save('data/processed/X_val.npy', X_val)
np.save('data/processed/y_val.npy', y_val)
np.save('data/processed/X_test.npy', X_test)
np.save('data/processed/y_test.npy', y_test)
print("\n‚úì Features salvas em data/processed/")
```

---

## 5Ô∏è‚É£ Treinamento Random Forest

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

print("="*80)
print("TREINAMENTO: RANDOM FOREST")
print("="*80)

# Achatar features (RF precisa de features 1D)
X_train_flat = X_train.reshape(X_train.shape[0], -1)
X_val_flat = X_val.reshape(X_val.shape[0], -1)
X_test_flat = X_test.reshape(X_test.shape[0], -1)

print(f"\nShapes achatadas:")
print(f"  Train: {X_train_flat.shape}")
print(f"  Val:   {X_val_flat.shape}")
print(f"  Test:  {X_test_flat.shape}")

# Treinar modelo
print("\nTreinando Random Forest...")
modelo_rf = RandomForestClassifier(
    n_estimators=150,
    max_depth=20,
    random_state=42,
    n_jobs=-1,
    verbose=2
)

modelo_rf.fit(X_train_flat, y_train)
print("\n‚úì Treinamento conclu√≠do!")

# Avaliar
print("\nAvaliando modelo...")
rf_train_acc = accuracy_score(y_train, modelo_rf.predict(X_train_flat))
rf_val_acc = accuracy_score(y_val, modelo_rf.predict(X_val_flat))
rf_test_acc = accuracy_score(y_test, modelo_rf.predict(X_test_flat))

print(f"\nüìä Resultados Random Forest:")
print(f"  Acur√°cia Treino: {rf_train_acc:.4f} ({rf_train_acc*100:.2f}%)")
print(f"  Acur√°cia Val:    {rf_val_acc:.4f} ({rf_val_acc*100:.2f}%)")
print(f"  Acur√°cia Teste:  {rf_test_acc:.4f} ({rf_test_acc*100:.2f}%)")

# Salvar modelo
!mkdir -p models
with open('models/random_forest.pkl', 'wb') as f:
    pickle.dump(modelo_rf, f)
print("\n‚úì Modelo salvo em models/random_forest.pkl")
```

```python
# Matriz de confus√£o Random Forest
from sklearn.metrics import confusion_matrix
import seaborn as sns

y_pred_rf = modelo_rf.predict(X_test_flat)
cm_rf = confusion_matrix(y_test, y_pred_rf)

plt.figure(figsize=(12, 10))
sns.heatmap(cm_rf, annot=False, cmap='Blues', cbar=True)
plt.title('Matriz de Confus√£o - Random Forest')
plt.ylabel('Verdadeiro')
plt.xlabel('Predito')
plt.tight_layout()
plt.show()

# Relat√≥rio de classifica√ß√£o
print("\nRelat√≥rio de Classifica√ß√£o (Random Forest):")
print("="*60)
print(classification_report(y_test, y_pred_rf))
```

---

## 6Ô∏è‚É£ Treinamento CNN 1D

```python
from tensorflow import keras
from tensorflow.keras import layers

print("="*80)
print("TREINAMENTO: CNN 1D")
print("="*80)

# Construir modelo CNN
print("\nConstruindo arquitetura CNN...")

modelo_cnn = keras.Sequential([
    # Input
    keras.Input(shape=(X_train.shape[1], X_train.shape[2])),
    
    # Bloco Convolucional 1
    layers.Conv1D(64, kernel_size=3, padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.3),
    
    # Bloco Convolucional 2
    layers.Conv1D(128, kernel_size=3, padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.MaxPooling1D(pool_size=2),
    layers.Dropout(0.3),
    
    # Bloco Convolucional 3
    layers.Conv1D(256, kernel_size=3, padding='same'),
    layers.BatchNormalization(),
    layers.Activation('relu'),
    layers.GlobalAveragePooling1D(),
    layers.Dropout(0.5),
    
    # Camadas Densas
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(dataset.get_num_speakers(), activation='softmax')
])

# Compilar
modelo_cnn.compile(
    optimizer=keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("\nüìê Arquitetura do Modelo:")
modelo_cnn.summary()

# Callbacks
callbacks = [
    keras.callbacks.EarlyStopping(
        monitor='val_loss',
        patience=15,
        restore_best_weights=True,
        verbose=1
    ),
    keras.callbacks.ReduceLROnPlateau(
        monitor='val_loss',
        factor=0.5,
        patience=5,
        min_lr=1e-6,
        verbose=1
    ),
    keras.callbacks.ModelCheckpoint(
        'models/cnn_melhor.h5',
        monitor='val_accuracy',
        save_best_only=True,
        verbose=1
    )
]

# Treinar
print("\nTreinando CNN...")
print("‚ö†Ô∏è  Isso pode levar v√°rios minutos...\n")

historico = modelo_cnn.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    callbacks=callbacks,
    verbose=1
)

print("\n‚úì Treinamento conclu√≠do!")
```

```python
# Visualizar curvas de treinamento
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))

# Acur√°cia
ax1.plot(historico.history['accuracy'], label='Treino', linewidth=2)
ax1.plot(historico.history['val_accuracy'], label='Valida√ß√£o', linewidth=2)
ax1.set_xlabel('√âpoca')
ax1.set_ylabel('Acur√°cia')
ax1.set_title('Acur√°cia Durante o Treinamento')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Perda
ax2.plot(historico.history['loss'], label='Treino', linewidth=2)
ax2.plot(historico.history['val_loss'], label='Valida√ß√£o', linewidth=2)
ax2.set_xlabel('√âpoca')
ax2.set_ylabel('Perda')
ax2.set_title('Perda Durante o Treinamento')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()
```

```python
# Avaliar CNN
print("\nAvaliando CNN...")
cnn_train_loss, cnn_train_acc = modelo_cnn.evaluate(X_train, y_train, verbose=0)
cnn_val_loss, cnn_val_acc = modelo_cnn.evaluate(X_val, y_val, verbose=0)
cnn_test_loss, cnn_test_acc = modelo_cnn.evaluate(X_test, y_test, verbose=0)

print(f"\nüìä Resultados CNN 1D:")
print(f"  Acur√°cia Treino: {cnn_train_acc:.4f} ({cnn_train_acc*100:.2f}%)")
print(f"  Acur√°cia Val:    {cnn_val_acc:.4f} ({cnn_val_acc*100:.2f}%)")
print(f"  Acur√°cia Teste:  {cnn_test_acc:.4f} ({cnn_test_acc*100:.2f}%)")

# Salvar modelo
modelo_cnn.save('models/cnn_modelo.h5')
print("\n‚úì Modelo salvo em models/cnn_modelo.h5")
```

```python
# Matriz de confus√£o CNN
y_pred_cnn = np.argmax(modelo_cnn.predict(X_test), axis=1)
cm_cnn = confusion_matrix(y_test, y_pred_cnn)

plt.figure(figsize=(12, 10))
sns.heatmap(cm_cnn, annot=False, cmap='Greens', cbar=True)
plt.title('Matriz de Confus√£o - CNN 1D')
plt.ylabel('Verdadeiro')
plt.xlabel('Predito')
plt.tight_layout()
plt.show()

# Relat√≥rio de classifica√ß√£o
print("\nRelat√≥rio de Classifica√ß√£o (CNN):")
print("="*60)
print(classification_report(y_test, y_pred_cnn))
```

---

## 7Ô∏è‚É£ An√°lise Comparativa

```python
print("="*80)
print("COMPARA√á√ÉO FINAL: RANDOM FOREST vs CNN 1D")
print("="*80)
print()

# Criar tabela de compara√ß√£o
comparacao = pd.DataFrame({
    'Modelo': ['Random Forest', 'CNN 1D'],
    'Acur√°cia Treino': [rf_train_acc, cnn_train_acc],
    'Acur√°cia Valida√ß√£o': [rf_val_acc, cnn_val_acc],
    'Acur√°cia Teste': [rf_test_acc, cnn_test_acc]
})

print(comparacao.to_string(index=False))
print()

# Determinar vencedor
if cnn_test_acc > rf_test_acc:
    vencedor = "CNN 1D"
    diferenca = (cnn_test_acc - rf_test_acc) * 100
else:
    vencedor = "Random Forest"
    diferenca = (rf_test_acc - cnn_test_acc) * 100

print(f"üèÜ VENCEDOR: {vencedor}")
print(f"   Diferen√ßa: {diferenca:.2f}% pontos percentuais")
```

```python
# Gr√°fico de barras comparativo
fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(3)
largura = 0.35

bars1 = ax.bar(x - largura/2, 
               [rf_train_acc, rf_val_acc, rf_test_acc],
               largura, label='Random Forest', color='steelblue', alpha=0.8)

bars2 = ax.bar(x + largura/2,
               [cnn_train_acc, cnn_val_acc, cnn_test_acc],
               largura, label='CNN 1D', color='coral', alpha=0.8)

ax.set_xlabel('Conjunto de Dados', fontsize=12)
ax.set_ylabel('Acur√°cia', fontsize=12)
ax.set_title('Compara√ß√£o de Desempenho: RF vs CNN', fontsize=14, fontweight='bold')
ax.set_xticks(x)
ax.set_xticklabels(['Treino', 'Valida√ß√£o', 'Teste'])
ax.legend(fontsize=11)
ax.grid(True, alpha=0.3, axis='y')
ax.set_ylim([0, 1.1])

# Adicionar valores nas barras
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
                f'{height:.3f}',
                ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.show()
```

---

## 8Ô∏è‚É£ Resultados e Conclus√µes

```python
print("="*80)
print("RELAT√ìRIO FINAL")
print("="*80)
print()

print("üìä DESEMPENHO DOS MODELOS")
print("-" * 80)
print(f"\nRandom Forest:")
print(f"  ‚îú‚îÄ Acur√°cia no teste: {rf_test_acc*100:.2f}%")
print(f"  ‚îú‚îÄ Overfitting: {(rf_train_acc - rf_test_acc)*100:.2f}% pontos")
print(f"  ‚îî‚îÄ Tempo de treinamento: R√°pido (< 1 min)")

print(f"\nCNN 1D:")
print(f"  ‚îú‚îÄ Acur√°cia no teste: {cnn_test_acc*100:.2f}%")
print(f"  ‚îú‚îÄ Overfitting: {(cnn_train_acc - cnn_test_acc)*100:.2f}% pontos")
print(f"  ‚îî‚îÄ Tempo de treinamento: Moderado (5-15 min)")

print(f"\nüèÜ Modelo Vencedor: {vencedor}")
print(f"   Superioridade: {diferenca:.2f}% pontos percentuais")

print("\nüí° CONCLUS√ïES")
print("-" * 80)

if vencedor == "CNN 1D":
    print("""
A CNN 1D demonstrou melhor capacidade de:
  ‚úì Capturar padr√µes temporais nas features de √°udio
  ‚úì Generalizar para dados n√£o vistos
  ‚úì Aprender representa√ß√µes hier√°rquicas
  
Recomenda√ß√£o: Usar CNN 1D para produ√ß√£o
""")
else:
    print("""
O Random Forest demonstrou melhor desempenho devido a:
  ‚úì Efici√™ncia com datasets menores
  ‚úì Menor propens√£o a overfitting
  ‚úì Treinamento mais r√°pido
  ‚úì N√£o requer ajuste fino extenso
  
Recomenda√ß√£o: RF para prototipagem r√°pida, CNN com mais dados
""")

print("\nüìÅ ARQUIVOS GERADOS")
print("-" * 80)
print("  ‚îú‚îÄ models/random_forest.pkl")
print("  ‚îú‚îÄ models/cnn_modelo.h5")
print("  ‚îú‚îÄ models/cnn_melhor.h5")
print("  ‚îî‚îÄ data/processed/*.npy")

print("\n‚úÖ PIPELINE COMPLETO EXECUTADO COM SUCESSO!")
print("="*80)
```

```python
# Salvar resultados finais
resultados_finais = {
    'random_forest': {
        'train_acc': float(rf_train_acc),
        'val_acc': float(rf_val_acc),
        'test_acc': float(rf_test_acc)
    },
    'cnn': {
        'train_acc': float(cnn_train_acc),
        'val_acc': float(cnn_val_acc),
        'test_acc': float(cnn_test_acc),
        'historico': historico.history
    },
    'vencedor': vencedor,
    'diferenca_percentual': float(diferenca),
    'num_falantes': dataset.get_num_speakers(),
    'num_amostras': len(dataset)
}

# Salvar
with open('results/resultados_finais.pkl', 'wb') as f:
    pickle.dump(resultados_finais, f)

print("‚úì Resultados salvos em results/resultados_finais.pkl")

# Download dos resultados
from google.colab import files
files.download('results/resultados_finais.pkl')
files.download('models/random_forest.pkl')
files.download('models/cnn_modelo.h5')
```

---

## üìù Notas Finais

- **Tempo estimado**: 30-60 minutos (depende do tamanho do dataset)
- **Recursos**: Recomendado usar GPU no Colab para CNN
- **Mem√≥ria**: M√≠nimo 12GB RAM recomendado
- **Dataset**: VoxCeleb1 completo ~38GB, usar apenas teste para testes r√°pidos

---

**Autor**: Projeto BCC406 - Redes Neurais  
**Data**: Dezembro 2025  
**Vers√£o**: 1.0
